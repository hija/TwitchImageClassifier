{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitch Image Classifier\n",
    "Originally, I did a project in the data analysis class in which we classified twitch images according to the shown game. Back then, we used SVMs and transfered learning.\n",
    "Now, I want to do this project again - this time building my own network architecture. My goal is to get at least 80% accuracy.\n",
    "\n",
    "## Setup\n",
    "Please add the twitch images to the data directory. If you do not have any preview images, then you can use my twitch preview image crawler, which is also available on my github.\n",
    "\n",
    "The following directory structure should be present\n",
    "\n",
    "data/training/Game1/...\n",
    "\n",
    "data/training/Game2/...\n",
    "\n",
    "data/training/Game...\n",
    "\n",
    "data/validation/Game1/...\n",
    "\n",
    "data/validation/Game2/...\n",
    "\n",
    "data/validataion/Game..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "### Check for networkx version\n",
    "For hyperas networkx version 1.11 is required. Thus, we need to check this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "\n",
    "assert networkx.__version__ == '1.11', 'You need to install networkx version 1.11, otherwise hyperas does not work'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check directory structure\n",
    "In the first step we'll just check, if the right directory structure is present. This will hopefully save some time, if I work on this later again and I need to setup the environment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# First check if data directory is present. This should be the case anyways, since this is set in the git repository,\n",
    "# but we'll check :)\n",
    "\n",
    "assert os.path.isdir('data'), 'The data directory is not present, but mandatory'\n",
    "\n",
    "# Check if training and validation is present\n",
    "assert os.path.isdir('data/training'), 'The training directory in the data directory is not present, but mandatory'\n",
    "assert os.path.isdir('data/validation'), 'The validation directory in the data directory is not present, but mandatory'\n",
    "\n",
    "# Check how many directories (== Games) there are in training and validation\n",
    "training_games = [game for game in os.listdir(\"data/training/\") if os.path.isdir('data/training/{}'.format(game))]\n",
    "validation_games = [game for game in os.listdir(\"data/validation/\") if os.path.isdir('data/validation/{}'.format(game))]\n",
    "assert training_games == validation_games, 'The games must be the same in training and validation'\n",
    "\n",
    "# Check how many games there are\n",
    "assert len(training_games) >= 1, 'You need to add at least one game'\n",
    "assert len(training_games) >= 2, 'For a real classification scenario you should add more than one game'\n",
    "\n",
    "# Determine training and validation sample size\n",
    "training_samples = len([name for name in glob.glob('data/training/*/*') if os.path.isfile(name)])\n",
    "validation_samples = len([name for name in glob.glob('data/validation/*/*') if os.path.isfile(name)])\n",
    "assert training_samples > validation_samples, 'You should use more training samples than validation samples'\n",
    "assert training_samples > 32, 'You should use at least 32 files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Modeldefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_generator, validation_generator):\n",
    "    \n",
    "    training_steps = training_samples // batch_size\n",
    "    validation_steps = validation_samples // batch_size\n",
    "    \n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_steps,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps)\n",
    "\n",
    "    score, acc = model.evaluate_generator(validation_generator)\n",
    "\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definition\n",
    "Here we define the data image size and the dataflows, so they can be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def definitions():\n",
    "    global batch_size, epochs, img_width, img_height, training_samples, validation_samples, nb_classes\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    img_width, img_height = 320, 180\n",
    "    \n",
    "    training_samples = len([name for name in glob.glob('data/training/*/*') if os.path.isfile(name)])\n",
    "    validation_samples = len([name for name in glob.glob('data/validation/*/*') if os.path.isfile(name)])\n",
    "    nb_classes = len([name for name in os.listdir('data/training') if os.path.isdir('data/training/{}'.format(name))])\n",
    "    \n",
    "\n",
    "def data():\n",
    "    # Load definitions\n",
    "    definitions()\n",
    "    \n",
    "    train_data_dir = 'data/training'\n",
    "    validation_data_dir = 'data/validation'\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=None)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=None)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main-Loop\n",
    "This is the main loop, starting keras with the hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import networkx\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import glob\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import cifar10\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "  1: def definitions():\n",
      "  2:     global batch_size, epochs, img_width, img_height, training_samples, validation_samples, nb_classes\n",
      "  3:     batch_size = 32\n",
      "  4:     epochs = 10\n",
      "  5:     img_width, img_height = 320, 180\n",
      "  6:     \n",
      "  7:     training_samples = len([name for name in glob.glob('data/training/*/*') if os.path.isfile(name)])\n",
      "  8:     validation_samples = len([name for name in glob.glob('data/validation/*/*') if os.path.isfile(name)])\n",
      "  9:     nb_classes = len([name for name in os.listdir('data/training') if os.path.isdir('data/training/{}'.format(name))])\n",
      " 10:     print(nb_classes)\n",
      " 11: \n",
      " 12: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: # Load definitions\n",
      "  3: definitions()\n",
      "  4: \n",
      "  5: train_data_dir = 'data/training'\n",
      "  6: validation_data_dir = 'data/validation'\n",
      "  7: \n",
      "  8: test_datagen = ImageDataGenerator(\n",
      "  9:     rescale=None)\n",
      " 10: \n",
      " 11: train_datagen = ImageDataGenerator(\n",
      " 12:     rescale=None)\n",
      " 13: \n",
      " 14: train_generator = train_datagen.flow_from_directory(\n",
      " 15:     train_data_dir,\n",
      " 16:     target_size=(img_width, img_height),\n",
      " 17:     batch_size=batch_size,\n",
      " 18:     class_mode='categorical')\n",
      " 19: \n",
      " 20: validation_generator = test_datagen.flow_from_directory(\n",
      " 21:     validation_data_dir,\n",
      " 22:     target_size=(img_width, img_height),\n",
      " 23:     batch_size=batch_size,\n",
      " 24:     class_mode='categorical')\n",
      " 25: \n",
      " 26: \n",
      " 27: \n",
      " 28: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     training_steps = training_samples // batch_size\n",
      "   5:     validation_steps = validation_samples // batch_size\n",
      "   6:     \n",
      "   7:     input_shape = (img_width, img_height, 3)\n",
      "   8:     \n",
      "   9:     model = Sequential()\n",
      "  10:     model.add(Conv2D(16, (3, 3), input_shape=input_shape))\n",
      "  11:     model.add(Activation('relu'))\n",
      "  12:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "  13:     \n",
      "  14:     model.add(Conv2D(32, (3, 3)))\n",
      "  15:     model.add(Activation('relu'))\n",
      "  16:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "  17: \n",
      "  18:     model.add(Conv2D(64, (3, 3)))\n",
      "  19:     model.add(Activation('relu'))\n",
      "  20:     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "  21: \n",
      "  22:     model.add(Flatten())\n",
      "  23:     model.add(Dense(64))\n",
      "  24:     model.add(Activation('relu'))\n",
      "  25:     model.add(Dropout(space['Dropout']))\n",
      "  26:     model.add(Dense(nb_classes))\n",
      "  27:     model.add(Activation('sigmoid'))\n",
      "  28:     model.compile(loss=['categorical_crossentropy'],\n",
      "  29:                   optimizer='adam',\n",
      "  30:                   metrics=['accuracy'])\n",
      "  31: \n",
      "  32:     model.fit_generator(\n",
      "  33:         train_generator,\n",
      "  34:         steps_per_epoch=training_steps,\n",
      "  35:         epochs=epochs,\n",
      "  36:         validation_data=validation_generator,\n",
      "  37:         validation_steps=validation_steps)\n",
      "  38: \n",
      "  39:     score, acc = model.evaluate_generator(validation_generator)\n",
      "  40: \n",
      "  41:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  42: \n",
      "10\n",
      "Found 4000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "  5/125 [>.............................] - ETA: 4:40 - loss: 6.9109 - acc: 0.1125"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=15,\n",
    "                                          trials=Trials(),\n",
    "                                          functions=[definitions],\n",
    "                                          notebook_name='TwitchImageClassifier')\n",
    "print(best_run)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
